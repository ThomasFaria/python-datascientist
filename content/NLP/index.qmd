---
title: "Partie 4 : Natural Language Processing (NLP)"
slug: "nlp"
categories:
  - Introduction
  - NLP
description: |
  L'un des grands avantages comparatifs de {{< fa brands python >}} par rapport aux
  langages concurrents ({{< fa brands r-project >}} notamment) est dans
  la richesse des librairies de traitement du langage naturel (mieux
  connu sous son acronyme anglais : NLP pour _natural langage processing_).
  Cette partie vise √† illustrer la richesse de cet √©cosyst√®me √† partir
  de quelques exemples litt√©raires :  Dumas, Poe, Shelley, Lovecraft.
image: https://minio.lab.sspcloud.fr/lgaliana/generative-art/pythonds/nlp.png
---

# Introduction

Les parties pr√©c√©dentes √©taient consacr√©es √† l'acquisition de comp√©tences
transversales pour la valorisation des donn√©es. De mani√®re naturelle, 
nous nous sommes jusqu'√† pr√©sent plut√¥t consacr√©s
√† la valorisation de donn√©es structur√©es, d'une
dimension certes modeste mais qui ouvraient d√©j√† √©norm√©ment de
probl√©matiques √† creuser. Cette partie propose maintenant de se
consacrer √† un sujet dont il n'est pas √©vident _a priori_ que
les ordinateurs s'emparent, source de d√©bats philosophiques s√©culaires, de Platon √† Saussure : le __langage humain__ et sa richesse. 

En faisant l'analogie entre langue et langage, c'est-√†-dire en d√©finissant ce dernier comme la capacit√© d'expression et de communication d'une pens√©e par le biais de signes et en d√©finissant la langue comme la mise en oeuvre conventionnelle de cette capacit√©, on peut se placer dans les traces de la linguistique
et repr√©senter le langage sous une forme de donn√©es. 
Ceci ouvre la voix √† l'analyse statistique ou algorithmique. N√©anmoins, m√™me s'il existe des r√©gularit√©s statistiques , comment
des ordinateurs, qui au fond ne connaissent que le _0_ et le _1_, peuvent-ils
s'approprier cet objet √©minemment complexe qu'est le langage et qu'un 
humain met lui-m√™me des ann√©es √† comprendre et s'approprier ?[^blague]

[^blague]: En √©crivant ces lignes j'ai demand√© √† `ChatGPT` de me faire une blague. 

    > Pourquoi les programmeurs d√©testent-ils la plage ?
    > Parce que le sable change tout le temps de paradigme !

    Ne l'ayant pas comprise, je lui
    ai demand√© de me l'expliquer. Voici sa r√©ponse :

    > Bien s√ªr, je serais ravi de l'expliquer ! Cette blague joue sur le double sens du mot "paradigme" et fait un jeu de mots avec les notions li√©es √† la programmation et √† la plage.
    >
    > Dans le domaine de la programmation, un "paradigme" d√©signe un mod√®le ou une m√©thode de programmation, comme la programmation orient√©e objet, la programmation fonctionnelle, etc. Les programmeurs sont souvent familiers avec diff√©rents paradigmes de programmation.
    >
    > D'autre part, en contexte de plage, "paradigme" est utilis√© pour signifier "mar√©e qui monte et descend", se r√©f√©rant √† l'action naturelle du sable et de l'eau sur la plage.
    >
    > La blague joue sur la frustration humoristique que peuvent ressentir les programmeurs lorsqu'ils doivent s'adapter √† de nouveaux paradigmes de programmation, tout comme le sable de la plage qui change constamment avec les mar√©es. En associant les deux sens du mot "paradigme", la blague cr√©e une situation absurde et amusante, faisant sourire ceux qui comprennent les r√©f√©rences √† la programmation et √† la plage.
   
    Finalement, j'ai trouv√© son explication plus dr√¥le que sa blague.  

# Le traitement automatique du langage 

Le traitement automatique du langage - traduction fran√ßaise du concept de
_natural language processing_ (NLP) - est l'ensemble des techniques
permettant aux ordinateurs de comprendre, analyser, synth√©tiser et
g√©n√©rer le langage humain[^1]. 

[^1]: Le concept de _natural language processing_ tend √† remplacer celui
de _text mining_ dont le p√©rim√®tre √©tait plus sp√©cifique. Le _text mining_
renvoie √† la recherche d'information - la fouille - dans un ensemble de 
textes. C'est l'un des champs d'applications
du _natural language processing_ mais ce n'est pas l'unique. L'accroissement des ressources de calcul et les progr√®s dans la formalisation du langage ont permis d'√©largir le champ des domaines o√π la linguistique computationnelle intervient.

Il s'agit d'un champ disciplinaire √† l'intersection de la statistique
et de la linguistique qui connait depuis quelques ann√©es un engouement
important, que ce soit d'un point de vue acad√©mique, op√©rationnel ou industriel. 
Certaines des applications de ces techniques sont devenues incontournables
dans nos t√¢ches quotidiennes, notamment les moteurs de recherche, la traduction
automatique et plus r√©cemment les _chatbots_ dont le d√©veloppement conna√Æt depuis l'√©mergence de `ChatGPT` en d√©cembre 2022 un rythme fou.

# R√©sum√© de la partie

Cette partie du cours est consacr√©e √† l'analyse des donn√©es textuelles avec
des exemples de üìñ pour s'amuser. Elle est une introduction progressive
√† ce sujet en se concentrant sur des concepts de base, n√©cessaires √†
la compr√©hension ult√©rieure de principes plus avanc√©s et de techniques
sophistiqu√©es[^2]. Cette partie pr√©sente principalement :

- Les enjeux de nettoyage de champs textuels
et d'analyse de fr√©quence. Il s'agit de NLP un
peu _old school_ mais dont la compr√©hension est n√©cessaire pour aller
plus loin ;
- La mod√©lisation du langage, selon plusieurs approches. 

[^2]: Par exemple, le concept d'_embedding_ - transformation d'un champ
textuel en un vecteur num√©rique multidimensionnel - aujourd'hui central
dans le NLP n'est √©voqu√© qu'√† quelques reprises. 

Avant d'en arriver
au sujet des _embeddings_, il est pr√©cieux de comprendre les apports et les
limites de concepts comme
le sac de mot (_bag of words_) ou la distance
TF-IDF (_term frequency - inverse document frequency_). L'un des apports principaux des grands mod√®les de langage, √† savoir la richesse de la f√™netre contextuelle leur permettant de mieux saisir les nuances textuelles et l'intentionalit√© du locuteur, s'√©clairent lorsqu'on saisit les limites du NLP traditionnel. 

Dans une
optique introductive, ce cours se focalise donc sur les approches fr√©quentistes, notamment l'approche sac de mot, pour faciliter
l'ouverture ult√©rieure de la boite de Pandore que sont les _embeddings_. 

## Nettoyages textuels et analyse de fr√©quences

`Python` est un excellent outil pour l'analyse de donn√©es textuelles. 
Les m√©thodes de base de transformation de donn√©es textuelles ou de dictionnaires, associ√©es √† des librairies sp√©cialis√©es
comme `NLTK` et `SpaCy`, permettent d'effectuer des t√¢ches de normalisation et d'analyse de donn√©es textuelles de mani√®re
tr√®s efficace. `Python` est bien mieux outill√© que `R` pour l'analyse de
donn√©es textuelles. 
Les ressources en ligne sur le sujet sont tr√®s 
nombreuses et la meilleure des √©coles dans le domaine reste la pratique sur un corpus √† nettoyer.  

Dans un premier temps, cette partie propose
de revenir sur la mani√®re de structurer et nettoyer un corpus 
textuel au travers de l'approche *bag of words* (sac de mots). 
Elle vise √† montrer comment transformer un corpus en outil propre √† une 
analyse statistique :

* Elle propose d'abord une introduction aux enjeux du nettoyage des donn√©es
textuelles √† travers l'analyse du *Comte de Monte Cristo* d'Alexandre Dumas
[ici](/content/NLP/01_intro.qmd) qui permet de synth√©tiser rapidement l'information disponible
dans un large volume de donn√©es (√† l'image de la @fig-wordcloud-dumas)
* Elle propose ensuite une s√©rie d'exercices sur le nettoyage de textes √† partir des
oeuvres d'Edgar Allan Poe, Mary Shelley et H.P. Lovecraft visant √† distinguer la 
sp√©cificit√© du vocabulaire employ√© par chaque auteurs (par exemple @fig-waffle-fear). Ces exercices sont 
disponibles [dans le deuxi√®me chapitre](/content/NLP/01_exoclean.html) de la partie.

Cette analyse fr√©quentiste permet de prendre du recul sur la nature des donn√©es textuelles et sur les enjeux r√©currents dans la r√©duction de dimension de corpus en langue naturelle. Comme la statistique descriptive entra√Æne naturellement la mod√©lisation, cette approche fr√©quentiste va g√©n√©ralement amener rapidement √† vouloir synth√©tiser quelques lois derri√®re nos corpus textuels. 


## Mod√©lisation du langage

La suite de cette partie proposera une introduction aux enjeux de mod√©lisation
du langage. Ceux-ci sont tr√®s √† la mode du fait du succ√®s de `ChatGPT`. N√©anmoins, avant
d'en arriver aux grands mod√®les de langage (LLM), ces r√©seaux de neurone ayant des milliards de param√®tres et entra√Æn√©s sur des volumes massifs de donn√©es, il est n√©cessaire de passer par quelques mod√©lisations 
pr√©liminaires. 

Nous proposerons d'abord d'explorer une approche alternative, prenant en compte
le contexte d'apparition d'un mot. L'introduction √† la
_Latent Dirichlet Allocation_ (LDA) sera l'occasion de pr√©senter la mod√©lisation
de documents sous la forme de *topics*. Celle-ci est n√©anmoins pass√©e de mode au profit des m√©thodes li√©es au concept d'_embedding_.

Nous introduirons ainsi √† la fin de cette partie du cours les enjeux de la transformation de champs textuels
sous forme de vecteurs num√©riques. Pour cela, nous pr√©senterons le principe
de `Word2Vec` qui permet ainsi, par exemple,
malgr√© une distance syntaxique importante,
de dire que s√©mantiquement `Homme` et `Femme` sont proches.
Ce chapitre est une passerelle vers le concept d'_embedding_, v√©ritable
r√©volution r√©cente du NLP, et qui permet de rapprocher des corpus
non seulement sur leur proximit√© syntaxique (partagent-ils par exemple des mots
communs ?) mais aussi sur leur proximit√© s√©mantique (partagent-ils un th√®me ou un sens commun ?)[^embedding]. Ce passage par `Word2Vec` permettra aux curieux de pouvoir ensuite passer aux mod√®les de type _transformers_, les mod√®les faisant aujourd'hui office de r√©f√©rence dans le domaine du NLP.

[^embedding]: Un exemple d'int√©r√™t de ce type d'approche est la @fig-relevanc-table-embedding.



## Pour aller plus loin {-}

La recherche dans le domaine du NLP est tr√®s active. Il est donc recommand√©
de faire preuve de curiosit√© pour en apprendre plus car une ressource
unique ne compilera pas l'ensemble des connaissances, _a fortiori_ dans
un champ de recherche aussi dynamique que le NLP. 

Pour approfondir les comp√©tences √©voqu√©es dans ce cours, je recommande vivement 
ce [cours d'`HuggingFace`](https://huggingface.co/course/chapter1/2?fw=pt). 

Pour comprendre l'architecture interne d'un LLM,
ce [post de Sebastian Raschka](https://magazine.sebastianraschka.com/p/understanding-encoder-and-decoder)
est tr√®s utile. 


Ces chapitres n'√©puisent pas les cas d'usage du NLP pour les _data scientists_. Ils n'en sont que la surface √©merg√©e de l'iceberg. 
Par exemple,
dans le domaine de la statistique publique, un des principaux cas d'usage du NLP est l'utilisation
de techniques de classification automatique pour transformer des r√©ponses libres dans des questionnaires
en champs pr√©d√©finis dans une nomenclature. 
Il s'agit donc d'une adaptation, un peu sp√©cifique √† la statistique publique, grande utilisatrice de nomenclatures normalis√©es, de probl√©matiques de classification multi-niveaux. 

Voici un exemple sur un projet de classification automatis√©e des professions dans la typologie
des nomenclatures d'activit√©s (les PCS) √† partir d'un mod√®le entra√Æn√© par la librairie `Fasttext` :

::: {.content-visible when-format="html"}

```{ojs}
//| echo: false
viewof activite = Inputs.text( 
  {label: '', value: 'data scientist', width: 800}
)
```


```{ojs}
//| echo: false
d3.json(urlApe).then(res => {
  var IC, results;

  ({ IC, ...results } = res);

  IC = parseFloat(IC);

  const rows = Object.values(results).map(obj => {
    return `
    <tr>
      <td>${obj.code} | ${obj.libelle}</td>
      <td>${obj.probabilite.toFixed(3)}</td>
    </tr>
  `;
  }).join('');

  const confidenceRow = `<tr>
    <td colspan="2" style="text-align:left; "><em>Indice de confiance : ${IC.toFixed(3)}</em></td>
  </tr>`;

  const tableHTML = html`
  <table>
    <caption>
      Pr√©diction de l'activit√©
    </caption>
    <tr>
      <th style="text-align:center;">Libell√© (NA2008)</th>
      <th>Probabilit√©</th>
    </tr>
      ${rows}
      ${confidenceRow}
  </table>`;

  // Now you can use the tableHTML as needed, for example, inserting it into the DOM.
  // For example, assuming you have a container with the id "tableContainer":
  return tableHTML;
});
```

```{ojs}
//| echo: false
activite_debounce = debounce(viewof activite, 2000)
urlApe = `https://codification-ape-test.lab.sspcloud.fr/predict?nb_echos_max=3&prob_min=0&text_feature=${activite_debounce}`
```

```{ojs}
//| echo: false
import {debounce} from "@mbostock/debouncing-input"
```

:::

::: {.content-hidden when-format="html"}

```{python}
import requests
import pandas as pd

activite = "data scientist"
urlApe = f"https://codification-ape-test.lab.sspcloud.fr/predict?nb_echos_max=3&prob_min=0&text_feature=${activite}"
import requests
data = requests.get(urlApe).json()

# Extract 'IC' value
IC = data['IC']
data.pop('IC', None)

df = pd.DataFrame(data.values())
df['indice_confiance'] = IC
df
```

:::